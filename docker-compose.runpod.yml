# Docker Compose configuration for RunPod deployment
# This is for reference - RunPod typically uses single containers
version: '3.8'

services:
  fireredtts2:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: fireredtts2-runpod
    
    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONPATH=/workspace
      - TORCH_HOME=/workspace/cache/torch
      - HF_HOME=/workspace/cache/huggingface
      - TRANSFORMERS_CACHE=/workspace/cache/transformers
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      - OMP_NUM_THREADS=4
      - MKL_NUM_THREADS=4
    
    # Port mapping
    ports:
      - "7860:7860"  # Gradio interface
      - "8080:8080"  # Health check
    
    # Volume mounts
    volumes:
      - ./models:/workspace/models:rw
      - ./cache:/workspace/cache:rw
      - ./logs:/workspace/logs:rw
      - ./uploads:/workspace/uploads:rw
      - ./sessions:/workspace/sessions:rw
    
    # Resource limits
    mem_limit: 32g
    memswap_limit: 32g
    shm_size: 8g
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Restart policy
    restart: unless-stopped
    
    # Working directory
    working_dir: /workspace
    
    # Command override
    command: ["/workspace/start.sh"]

# Networks
networks:
  default:
    driver: bridge

# Volumes for persistent storage
volumes:
  models:
    driver: local
  cache:
    driver: local
  logs:
    driver: local